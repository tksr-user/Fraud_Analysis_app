{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be52319-7f79-420d-b0e9-01af49cdee35",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts/ai_model_comparison.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m full_response = response1 + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + response2\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Save the final output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martifacts/ai_model_comparison.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     66\u001b[39m     f.write(full_response)\n\u001b[32m     68\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAI Agent Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'artifacts/ai_model_comparison.txt'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "# Connect to local MLflow server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment = mlflow.get_experiment_by_name(\"Fraud_Detection_Comparison_v1\")\n",
    "df_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id]).head(9)\n",
    "\n",
    "# Step 1: Create the first part of the prompt (metrics comparison only)\n",
    "def create_comparison_prompt_part1(df, target_run_id):\n",
    "    target_row = df[df[\"run_id\"] == target_run_id].iloc[0]\n",
    "    prompt = (\n",
    "        f\"Compare the following ML models to the target model (Run ID: {target_run_id}). \"\n",
    "        \"Use accuracy, precision, and recall to analyze performance differences.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[\"run_id\"] == target_run_id:\n",
    "            continue\n",
    "        model_type = row.get(\"params.model\", \"N/A\")\n",
    "        prompt += f\"Model {idx + 1} ({model_type}):\\n\"\n",
    "        prompt += f\" - Run ID: {row['run_id']}\\n\"\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "            col = f\"metrics.{metric}\"\n",
    "            if col in row and pd.notnull(row[col]):\n",
    "                prompt += f\" - {metric.capitalize()}: {row[col]:.4f}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "\n",
    "    prompt += (\n",
    "        f\"--- TARGET MODEL ---\\n\"\n",
    "        f\"Target Model ({df[df['run_id'] == target_run_id].index[0] + 1}):\\n\"\n",
    "        f\" - Run ID: {target_run_id}\\n\"\n",
    "        f\" - Accuracy: {target_row['metrics.accuracy']:.4f}\\n\"\n",
    "        f\" - Precision: {target_row['metrics.precision']:.4f}\\n\"\n",
    "        f\" - Recall: {target_row['metrics.recall']:.4f}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Step 2: Second prompt â€“ ranking based on the first analysis\n",
    "def create_followup_prompt():\n",
    "    return (\n",
    "        \"\\nNow based on the above comparison, rank all the models from best to worst.\\n\"\n",
    "        \"Include both the model type and Run ID in the ranking and explain why you chose this order.\"\n",
    "    )\n",
    "\n",
    "# Load the model\n",
    "model = GPT4All(\"Llama-3.2-3B-Instruct-Q4_0.gguf\")\n",
    "\n",
    "# Get the target model\n",
    "target_run_id = df_runs.iloc[0][\"run_id\"]\n",
    "\n",
    "# === Part 1: Comparison only ===\n",
    "prompt1 = create_comparison_prompt_part1(df_runs, target_run_id)\n",
    "response1 = model.generate(prompt1, max_tokens=1024, temp=0.7)\n",
    "\n",
    "# === Part 2: Ranking and explanation ===\n",
    "prompt2 = response1 + create_followup_prompt()\n",
    "response2 = model.generate(prompt2, max_tokens=1024, temp=0.7)\n",
    "\n",
    "# Combine both responses\n",
    "full_response = response1 + \"\\n\\n\" + response2\n",
    "\n",
    "# Save the final output\n",
    "with open(\"artifacts/ai_model_comparison.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(full_response)\n",
    "\n",
    "print(\"AI Agent Response:\\n\")\n",
    "print(full_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc298c1-0705-4f98-9bcb-007788f3604b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
