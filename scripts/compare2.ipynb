{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557d412-7393-4c2c-98aa-f64713a943b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "# Connect to local MLflow server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "experiment = mlflow.get_experiment_by_name(\"Fraud_Detection_Comparison_v2\")\n",
    "\n",
    "# Get all runs from the experiment\n",
    "df_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "\"\"\"# Preview your data\n",
    "print(df_runs[[\n",
    "    \"run_id\", \"params.model\", \n",
    "    \"metrics.accuracy\", \"metrics.precision\", \"metrics.recall\",\n",
    "]])\"\"\"\n",
    "def create_comparison_prompt(df, target_run_id):\n",
    "    target_row = df[df[\"run_id\"] == target_run_id].iloc[0]\n",
    "    prompt = (\n",
    "        f\"Compare the following ML models to the target model (Run ID: {target_run_id}). \"\n",
    "        \"Use accuracy, precision, and recall. Then rank all models from best to worst and explain your ranking.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[\"run_id\"] == target_run_id:\n",
    "            continue  # ⛔ Skip printing the target model again\n",
    "\n",
    "        model_type = row.get(\"params.model\", \"N/A\")\n",
    "        prompt += f\"Model {idx + 1} ({model_type}):\\n\"\n",
    "        prompt += f\" - Run ID: {row['run_id']}\\n\"\n",
    "        prompt += f\" - Model Type: {row.get('params.model', 'N/A')}\\n\"\n",
    "        for metric in [\"accuracy\", \"precision\", \"recall\"]:\n",
    "            col = f\"metrics.{metric}\"\n",
    "            if col in row and pd.notnull(row[col]):\n",
    "                prompt += f\" - {metric.capitalize()}: {row[col]:.4f}\\n\"\n",
    "    prompt += \"\\n\"\n",
    "\n",
    "\n",
    "    prompt += (\n",
    "    f\"\\n--- TARGET MODEL METRICS ---\\n\"\n",
    "    f\"Target Model ({df[df['run_id'] == target_run_id].index[0] + 1}):\\n\"\n",
    "    f\" - Run ID: {target_run_id}\\n\"\n",
    "    f\" - Accuracy: {target_row['metrics.accuracy']:.4f}\\n\"\n",
    "    f\" - Precision: {target_row['metrics.precision']:.4f}\\n\"\n",
    "    f\" - Recall: {target_row['metrics.recall']:.4f}\\n\"\n",
    ")\n",
    "    prompt += (\n",
    "    \"\\nRank all models from best to worst compared to the target model \"\n",
    "    \"using both model name and Run ID for clarity.\"\n",
    ")\n",
    "\n",
    "    return prompt\n",
    "        \n",
    "   \n",
    "# Choose a specific run ID to compare others against (e.g., best Logistic Regression model)\n",
    "target_run_id = df_runs.iloc[0][\"run_id\"]  # or pick based on best f1_score, etc.\n",
    "\n",
    "# Load your local model — path must match your installed model\n",
    "model = GPT4All(\"Llama-3.2-3B-Instruct-Q4_0.gguf\")\n",
    "\n",
    "# Generate response from prompt\n",
    "prompt = create_comparison_prompt(df_runs, target_run_id)\n",
    "response = model.generate(prompt, max_tokens=2048, temp=0.7)\n",
    "\n",
    "with open(\"artifacts/ai_model_comparison.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response)\n",
    "\n",
    "print(\"AI Agent Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d5d9b-7ffd-44da-af5b-311d4a447ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
